<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width" />
    <link rel="shortcut icon" href="img/favicon.png" type="image/x-icon" />
    <title>gemtc.drugis.org</title>
    <link rel="stylesheet" type="text/css" href="js/bower_components/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" type="text/css" href="css/gemtc-drugis.css">
    <script type="text/javascript" src="js/bower_components/foundation/js/vendor/custom.modernizr.js"></script>
    <script type="text/javascript" src="js/bower_components/bowser/bowser.min.js"></script>

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/katex.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.5.1/contrib/auto-render.min.js"></script>
  </head>
  <body>
    <div class="alert-box warning" style="display:none; margin-bottom: 0px;" id="browserCheck">
      Your browser, <span id="browserVersion1"></span>, is <b>out of date</b>.
      It has known <b>security flaws</b> and will <b>not run this web application correctly</b>.<br>
      <a href="http://browser-update.org/update-browser.html#drugis.org">Please update your browser</a> before continuing.
    </div>
    <div class="alert-box warning" style="display:none; margin-bottom: 0px;" id="browserUnknown">
      Your browser is unknown to us.
      This web application may or may not work correctly using it.
      Proceed at your own risk, or <a href="http://browser-update.org/update-browser.html#drugis.org">download a well-known browser</a> before continuing.
    </div>
    <script type="text/javascript">
    if (bowser.c || (bowser.msie && bowser.version <= 8)) {
      document.getElementById("browserVersion1").innerHTML = bowser.name + " " + bowser.version;
      document.getElementById("browserCheck").style.display = "block";
    }
    if (bowser.x) {
      document.getElementById("browserUnknown").style.display = "block";
    }
    </script>

    <section id="hero">
      <div class="row">
        <div class="columns large-offset-2 large-8 medium-12">
          <h1>GeMTC Manual</h1>

          <p style="margin-top: 30px; color: white;">Gert van Valkenhoef, Sylwia Bujkiewicz, Orestis Efthimiou, Daan Reid, and Connor Stroomberg</p>
        </div>
      </div>
    </section>

    <section class="content">
    	<div class="row">
    		<div class="columns large-offset-2 large-8 medium-12">
        <h2>Introduction</h2>

        <p>
        This is the manual for the GeMTC user interface for network meta-analysis.
        It starts with a brief introduction to network meta-analysis in the Bayesian framework, including issues such as model fit and convergence.
        This is followed by a guide to the GeMTC user interface itself.
        Terminology in the GeMTC user interface tends to be marked with a question mark icon, which when clicked will show a brief explanation of the term as well as a link to futher information in this manual.
        </p>

        <div style="text-align: center; margin-bottom: 1em;">
          <img src="img/glossary_popover.png" alt="glossary popover" style=" padding: 5px; border: 1px solid gray;">
        </div>

        <p>
        Although this manual contains a brief introduction to the terminology and methodology used in GeMTC, it is not a complete guide to network meta-analysis.
        For further background, we recommend the following excellent open access publications:
        </p>
        <ul>
          <li>The Medical Decision Making series on network meta-analysis based on the NICE Decision Support Unit series on evidence synthesis:
            <a href="http://mdm.sagepub.com/content/33/5.toc" title="Table of Contents">Table of Contents</a>,
            <a href="http://dx.doi.org/10.1177/0272989X12458724" title="S. Dias et al. (2012). Evidence Synthesis for Decision Making 2: A Generalized Linear Modeling Framework for Pairwise and Network Meta-analysis of Randomized Controlled Trials. Medical Decision Making.">MDM 2</a>,
            <a href="http://dx.doi.org/10.1177/0272989X13485157" title="S. Dias et al. (2012).Evidence Synthesis for Decision Making 3: Heterogeneity - Subgroups, Meta-Regression, Bias, and Bias-Adjustment. Medical Decision Making.">MDM 3</a>,
            <a href="http://dx.doi.org/10.1177/0272989X12455847" title="S. Dias et al (2012). Evidence Synthesis for Decision Making 4: Inconsistency in Networks of Evidence Based on Randomized Controlled Trials. Medical Decision Making.">MDM 4</a>
          </li>
          <li>The ISPOR guidelines on network meta-analysis:
            <a title="A.E. Ades (2011). ISPOR States Its Position on Network Meta-Analysis. Value in Health." href="http://dx.doi.org/10.1016/j.jval.2011.05.001">ISPOR 0</a>,
            <a title="J.P. Jansen et al. (2011). Interpreting Indirect Treatment Comparisons and Network Meta-Analysis for Health-Care Decision Making: Report of the ISPOR Task Force on Indirect Treatment Comparisons Good Research Practices: Part 1. Value in Health." href="http://dx.doi.org/10.1016/j.jval.2011.04.002">ISPOR 1</a>,
            <a title="D.C. Hoaglin et al. (2011). Conducting Indirect-Treatment-Comparison and Network-Meta-Analysis Studies: Report of the ISPOR Task Force on Indirect Treatment Comparisons Good Research Practices: Part 2. Value in Health." href="http://dx.doi.org/10.1016/j.jval.2011.01.011">ISPOR 2</a>
          </li>
          <li>Evaluating the quality of evidence from a network meta-analysis: <a href="10.1371/journal.pone.0099682" title="Evaluating the Quality of Evidence from a Network Meta-Analysis">Salanti et al. (2014)</a></li>
        </ul>

        <p>
        GeMTC is available as a stand-alone package at <a href="https://gemtc.drugis.org">gemtc.drugis.org</a>, and is also part of the <a href="https://addis.drugis.org">ADDIS</a> decision support system for evidence based medicine, developed at <a href="https://drugis.org">drugis.org</a>.
        The functionality in the GeMTC user interface is supported by the <a href="http://drugis.org/software/r-packages/gemtc">GeMTC R package</a>.
        All <a href="https://drugis.org">drugis.org</a> software is open source, and source code is available on <a href="https://github.com/drugis/">github.com/drugis</a>.
        </p>

        <h2>Bayesian meta-analysis</h2>

        <p>
        This section introduces the Bayesian models for meta-analysis used by GeMTC, starting with models for pair-wise meta-analysis and then moving on to network meta-analysis as an extension.
        The related issues of heterogeneity and inconsistency are discussed subsequently.
        We follow up with general techniques and considerations for Bayesian analysis as they apply within GeMTC: likelihood/link, prior distributions, model fit, and run-length and convergence.
        </p>

        <h3>General model structure</h3>

        <p>TODO: Sylwia's text.</p>

        <h3>Heterogeneity and inconsistency</h3>

        <p>
        In any meta-analysis, a key worry is whether the trials are similar enough in terms of potential <em>effect modifying covariates</em> to warrant the assumption that the true underlying trial effects are either identical (fixed effect) or exchangeable (random effects).
        Importantly, because the models synthesize treatment contrasts (relative effects), only covariates that differentially impact the different treatments are relevant.
        If a covariate (e.g. age at baseline) adversely affects the outcome irrespective of the intervention, it is not an effect modifier in the meta-analysis, assuming that the studies were properly randomized.
        </p>

        <p>
        In a pair-wise meta-analysis, a lack of similarity between trials can only manifest as heterogeneity, i.e. between-trials differences in the underlying treatment effects within a comparison.
        In network meta-analysis, it can also manifest as inconsistency, i.e. between-trial differences in the underlying treatment effects between comparisons.
        Although both share a common cause, inconsistency is more concerning as can be more difficult or even impossible to detect.
        The statistical assessment of heterogeneity is only limited by the number and precision of study estimates that are available.
        By contrast, the assessment of inconsistency is also limited by network structure: it can only be assessed if closed loops exist within the network, so that estimates arrived at by different routes can be compared.
        </p>

        <p>
        Therefore, in any meta-analysis, but especially in network meta-analysis, it is important to first consider whether the studies should be combined <em>at all</em>, based on epidemiological judgment of the study characteristics.
        Many reasoning aids exist to help in this assessment process, see <a href="http://dx.doi.org/10.1002/jrsm.1037" title="G. Salanti (2012). Indirect and mixed-treatment comparison, network, or multiple-treatments meta-analysis: many names, many benefits, many concerns for the next generation evidence synthesis tool. Research Synthesis Methods.">Salanti (2012)</a>.
        This should be supplemented by statistical tools, such as the node-splitting model for the assessment of inconsistency [TODO link to section].
        Heterogeneity and inconsistency are also affected by the scale of measurement chosen for the analysis, e.g. an analysis on the odds ratio scale might be more consistent than an analysis on the risk ratio scale (<a href="http://dx.doi.org/10.1002/sim.1188" title="J.J. Deeks (2002). Issues in the selection of a summary statistic for meta-analysis of clinical trials with binary outcomes. Statistics in Medicine.">Deeks, 2002</a>; <a href="http://dx.doi.org/10.1002/jrsm.1040" title="D.M. Caldwell et al. (2012). Selecting the best scale for measuring treatment effect in a network meta-analysis: a case study in childhood nocturnal enuresis. Research Synthesis Methods.">Caldwell et al., 2012</a>).
        The scale of measurement is discussed in the next section.
        </p>

        <p>
        See these references (<a href="http://dx.doi.org/10.1186/1741-7015-11-159" title="J.P. Jansen & H. Naci (2013). Is network meta-analysis as valid as standard pairwise meta-analysis? It all depends on the distribution of effect modifiers. BMC Medicine.">Jansen &amp; Naci, 2013</a>; <a href="http://dx.doi.org/10.1136/bmj.309.6965.1351" title="S.G. Thompson(1994). Why sources of heterogeneity in meta-analysis should be investigated. BMJ.">Thompson, 1994</a>; TODO) for a more in depth discussion of heterogeneity and inconsistency.
        </p>

        <h3>Likelihood / link</h3>

        <p>
        The likelihood reflects the distributional assumptions we make about the outcome data we aim to model.
        For example, a Binomial likelihood is usually used for count data (e.g. number of treatment responders).
        The link function transforms the parameters of the likelihood to the scale of measurement on which we assume the treatment effects to be linearly additive (<a href="http://dx.doi.org/10.1016/j.jval.2012.11.012" title="G. van Valkenhoef &amp; A.E. Ades (2013). Evidence synthesis assumes additivity on the scale of measurement: response to 'rank reversal in indirect comparisons' by Norton et al.. Value in Health.">van Valkenhoef &amp; Ades, 2013</a>).
        For example, the logit link transforms the Binomial success probability to log odds, resulting in an analysis on the log odds ratio scale.
        </p>

        <table>
          <thead>
            <caption>Likelihood / links supported by GeMTC</caption>
          </thead>
          <tr>
            <th>Type of data</th>
            <th>Likelihood</th>
            <th>Link</th>
            <th>Scale of measurement</th>
            <th>Remarks</th>
          </tr>
          <tr>
            <td>Count data</td>
            <td>Binomial</td>
            <td>Logit</td>
            <td>Log odds ratio</td>
            <td>-</td>
          </tr>
          <tr>
            <td>Count data</td>
            <td>Binomial</td>
            <td>Log</td>
            <td>Log risk ratio</td>
            <td>[TODO Warn et al. 2002]</td>
          </tr>
          <tr>
            <td>Count data</td>
            <td>Binomial</td>
            <td>Cloglog</td>
            <td>Log hazard ratio</td>
            <td>Treats counts as rate data by assuming equal follow-up in each arm</td>
          </tr>
          <tr>
            <td>Normal data</td>
            <td>Normal</td>
            <td>Identity</td>
            <td>Mean difference</td>
            <td>-</td>
          </tr>
          <tr>
            <td>Rate data</td>
            <td>Poisson</td>
            <td>Log</td>
            <td>Log hazard ratio</td>
            <td>-</td>
          </tr>
        </table>

        <h3>Prior distributions</h3>

        <p>
        In a Bayesian analysis, prior beliefs (represented by prior distributions) are combined with data to arrive at the posterior distribution.
        Quite often, a conservative analysis is desired with very little influence of prior beliefs.
        In that case, <em>vague</em> priors are assigned.
        In other cases, we may have strong prior beliefs (perhaps due to experience in clinical practice) which may be captured using <em>informative</em> priors.
        </p>

        <p>
        Prior distributions must be assigned to all top level parameters in a Bayesian model.
        For network meta-analysis, this typically means the treatment effects compared to a given reference treatment, the study baseline effects, and the heterogeneity variance in random effects models.
        If the distributions are sufficiently vague they are unlikely to have a noticable impact on model results, especially if sufficient data are available.
        In most situations, GeMTC can set the prior distributions automatically; the most straight-forward way to ensure they are sufficiently vague is to set the outcome scale parameter [TODO refer to section].
        The heterogeneity variance is often the most difficult parameter to estimate, and therefore sees the most impact of priors.
        In sparse networks, it may be sensible to assign an informative prior informed by empirical evidence
        (<a href="http://dx.doi.org/10.1093/ije/dys041" title="Turner et al. (2012). Predicting the extent of heterogeneity in meta-analysis, using empirical data from the Cochrane Database of Systematic Reviews. International Journal of Epidemiology.">Turner et al. 2012</a>; <a href="http://dx.doi.org/10.1016/j.jclinepi.2014.08.012" title="Rhodes et al. (2015). Predictive distributions were developed for the extent of heterogeneity in meta-analyses of continuous outcome data. Journal of Clinical Epidemiology.">Rhodes et al. 2015</a>; <a href="http://dx.doi.org/10.1002/sim.6381" title="Turner et al. (2015). Predictive distributions for between-study heterogeneity and simple methods for their application in Bayesian meta-analysis. Statistics in Medicine.">Turner et al. 2015</a>).
        </p>

        <h3>Model fit</h3>

        <p>
        Model fit (<a href="http://dx.doi.org/10.1111/1467-9868.00353" title="D.J. Spiegelhalter et al. (2002). Bayesian measures of model complexity and fit. Journal of the Royal Statistical Society: Series B (Statistical Methodology).">Spiegelhalter et al., 2002</a>) refers to how well the model represents the data, which is usually assessed by looking at the reverse: how likely the data are given the fitted model.
        Model fit statistics are useful to identify outliers, and to select the best fitting model from among a number of equally plausible models.
        </p>

        <p>
        A key metric of model fit is the <em>residual deviance</em> \(\bar{D}_{res}\), the posterior mean of the deviance under the current model, minus the deviance under the saturated model (i.e. a model with one parameter per data point).
        In a well fitting model, each data point can be expected to contribute about 1 to the residual deviance, so that we can expect \(\bar{D}_{res}\) to equal the number of independent data points (i.e. the number of arms).
        Lower values of residual deviance are better.
        The residual deviance of individual data points is often useful to identify outliers that contribute disproportionately to the overall residual deviance.
        </p>

        <p>
        Leverage statistics are used to assess the influence each data point has on the model parameters.
        In Bayesian models, the leverage of a data point is the posterior mean of the residual deviance minus the residual deviance at the posterior mean of the fitted value for that data point.
        The overall leverage \(p_D\) (the sum of the leverages of each data point) is also termed the effective number of parameters, and is a measure of model complexity.
        </p>

        <p>
        The deviance information criterion (DIC), defined as \( \bar{D}_{res} + p_D \), is a measure of model fit that penalizes model complexity.
        The DIC is used to compare fit between models for the same data; differences in DIC of 3 or greater are often considered relevant.
        </p>

        <h3>Run length and convergence</h3>
        <p>Bayesian inference using Markov Chain Monte Carlo relies on a “random walk” algorithm to sample from the posterior distribution implied by the data and the priors. The process starts by setting arbitrary values for the model parameters and continues by updating them  in every iteration. When assessing convergence we are concerned by two things:</p>
        <ol>
          <li>That the arbitrary starting values do not have an undue influence on the sampling process.</li>
          <li>That the quantities of interest have been estimated to sufficient accuracy.</li>
        </ol>
        <p>To address the first issue the first NB iterations, called the burn-in period, are discarded from the sample. In addition, the number of inference iterations, NI, must be sufficient to explore most of the parameter space. In order to assess the first condition it helps to compare early iterations to later iterations and to compare the results of independent runs (“chains”) with different starting values. For example, in the time series plot on the left, the four different chains (dashed green, blue, red, and black lines) follow clearly distinguishable paths, and their moving averages (solid lines) are clearly different between the first and second half of samples:</p>

        <div class="row">
          <div class="columns large-6 ">
            <img src="img/time_series_bad.png" alt="time series bad" >
          </div>
          <div class="columns large-6 ">
             <img src="img/time_series_good.png" alt="time series good">
          </div>
        </div>

        <p>In this case, it would be wise to increase both the number of burn-in iterations (NB) and the number of inference iterations, which leads to much better results, as illustrated by the time series plot on the right. The Brooks-Gelman-Rubin diagnostic incorporates these aspects in a numeric measure, the “Potential Scale Reduction Factor” (PSRF), which compares the variation within each chain to the variation between chains. The PSRF starts with a high value, and slowly approaches 1.0 as the chains become more similar. As a rule of thumb, the PSRF should at least be below 1.05. Plots can also show whether the PSRF is stable, as illustrated below.</p>

        <p>Again, the plot on the left shows a run with insufficient burn-in and inference iterations, where the</p>

        <div class="row">
          <div class="columns large-6 ">
            <img src="img/psrf_bad.png" alt="psrf plot bad">
          </div>
          <div class="columns large-6 ">
             <img src="img/psrf_good.png" alt="psrf plot good" >
          </div>
        </div>

        <p>PSRF is unstable and has high values. On the right, the PSRF steadily decreases and rapidly reaches stable values below 1.01.</p>

        <p>The second issue, i.e. that the number of iterations should be enough to estimate the quantities of interest with sufficient accuracy, depends on the first condition being met. If this is the case, the Monte Carlo error provides an idea of the accuracy of the estimates, and should be small compared to the standard deviation of the quantities being estimated. In addition, the time series plots can give some idea of the estimation accuracy, as can the density plots.</p>

        <div class="row">
          <div class="columns large-6 ">
            <img src="img/density_good.png" alt="Density plot bad" >
          </div>
          <div class="columns large-6 ">
             <img src="img/density_good.png" alt="Density plot good" >
          </div>
        </div>

        <p>In the density plots above, the figure on the left has outlying values in the tail of the distribution (the small vertical lines at the bottom, indicating sampled values), and doesn’t have an entirely regular shape. The density on the right is much smoother and has a more expected tapering off of extreme values in the tails.</p>

        <h2 id="node-splitting-model">Node-splitting analysis of inconsistency</h2>

        <p>The node-splitting approach for assessing the inconsistency of a network of interventions was introduced by <a href="http://dx.doi.org/10.1002/sim.3767" title="S. Dias et al. (2010). Checking consistency in mixed treatment comparison meta-analysis. Statistics in Medicine.">Dias et al. (2010)</a>. The method focuses on one treatment comparison at a time. In this approach the direct evidence for a specific treatment comparison (i.e. the evidence coming from RCTs that directly perform this comparison) is contrasted with the indirect evidence coming from the rest of the network. </p>

        <p>This is depicted in the figure below, for a network of eight treatments (A to H), where the node-splitting analysis is performed for the comparison of A vs. B. Studies directly comparing A to B are removed from the network and are used to obtain a direct estimate of the corresponding relative treatment effects, . Information coming from the rest of the network is used to obtain an indirect estimate of A vs. B,  These two estimates can then be contrasted, with important differences corresponding to the notion of network inconsistency. The difference between the direct and the indirect estimate is usually termed inconsistency factor, ; a posterior distribution of  close to zero implies no important discrepancies between direct and indirect evidence, and thus no inconsistency in the network. Values of away from zero indicate the existence of an inconsistency in the network.</p>

        <p>The node-splitting method can be applied for all treatment comparisons for which there is direct evidence available in the network. This means that if for example there are no X vs. Y studies, one cannot perform the node-splitting method for the XY comparison.</p>

        <img src="img/network.png" alt="networkgraph nodesplit example" >

        <h2>Network meta-analysis in GeMTC</h2>

        <p>The following sections give an overview of the GeMTC user interface.<p>

        <h3>Preparing your dataset</h3>
        <p>This section only applies to the stand-alone version of GeMTC, hosted on <a href="https://gemtc.drugis.org">https://gemtc.drugis.org/</a>. In ADDIS (<a href="https://addis.drugis.org/">https://addis.drugis.org/</a>), this step is not necessary.</p>
        <p>After signing in to GeMTC, you will be redirected to your personal home page. It contains a list of your previously created analyses (which will be empty until you create one), and a button to create a new analysis. Clicking this button will open the “New analysis” dialog, where you choose a title for your analysis and the outcome, and upload a dataset file:</p>

        <img src="img/gemtc_add_analysis.png" alt="add analysis screen shot" >

        <p>Datasets can be uploaded in CSV format. The CSV file should contain one row per arm, and needs to contain “study” and “treatment” columns, which can contain names or identifiers for the study and treatment. The data columns may be one of:</p>
        <ul>
          <li>Continuous data: “mean” and “std.err”</li>
          <li>Continuous data: “mean”, “std.dev”, and “sampleSize”</li>
          <li>Dichotomous data: “responders” and “sampleSize”</li>
          <li>Survival data: “responders” and “exposure” (exposure in person-time)</li>
        </ul>

        <p>The following is an example CSV file for a Parkinson’s disease dataset:</p>

        <pre>
          "study","treatment","mean","std.dev","sampleSize"
          "1","A",-1.22,3.7,54
          "1","C",-1.53,4.28,95
          "2","A",-0.7,3.7,172
          "2","B",-2.4,3.4,173
          "3","A",-0.3,4.4,76
          "3","B",-2.6,4.3,71
          "3","D",-1.2,4.3,81
          "4","C",-0.24,3,128
          "4","D",-0.59,3,72
          "5","C",-0.73,3,80
          "5","D",-0.18,3,46
          "6","D",-2.2,2.31,137
          "6","E",-2.5,2.18,131
          "7","D",-1.8,2.48,154
          "7","E",-2.1,2.99,143
        </pre>

        <h3>Analyses and models</h3>
        <p>TODO</p>
        <h3>Creating a new model</h3>
        <p>TODO</p>
        <h3>Interpreting model results</h3>
        <p>TODO</p>
        <p>- Different sections; what is in each section?
        - Link to node-split overview
        - Refer to explanatory sections on convergence and model fit
        - Examples of relative effects / rank probabilities</p>

      </div>
    </div>
   </div>

   <script>
    renderMathInElement(document.body);
   </script>
  </body>
</html>
